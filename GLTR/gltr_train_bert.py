# -*- coding: utf-8 -*-
"""GLTRipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1Yu-h2xEqTa8NztABEGtNv-BNSx8YlroF
"""

import numpy as np
import torch
import time
import jsonlines
from sklearn.linear_model import LogisticRegression
from transformers import (GPT2LMHeadModel, GPT2Tokenizer,
							BertTokenizer, BertForMaskedLM)
from class_register import register_api
from random import random
import argparse
import transformers
from sklearn.metrics import precision_recall_fscore_support as score
import pickle
from sklearn import metrics
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

class AbstractLanguageChecker():
	"""
	Abstract Class that defines the Backend API of GLTR.

	To extend the GLTR interface, you need to inherit this and
	fill in the defined functions.
	"""

	def __init__(self):
		'''
		In the subclass, you need to load all necessary components
		for the other functions.
		Typically, this will comprise a tokenizer and a model.
		'''
		self.device = torch.device(
			"cuda" if torch.cuda.is_available() else "cpu")

	def check_probabilities(self, in_text, topk=40):
		'''
		Function that GLTR interacts with to check the probabilities of words

		Params:
		- in_text: str -- The text that you want to check
		- topk: int -- Your desired truncation of the head of the distribution

		Output:
		- payload: dict -- The wrapper for results in this function, described below

		Payload values
		==============
		bpe_strings: list of str -- Each individual token in the text
		real_topk: list of tuples -- (ranking, prob) of each token
		pred_topk: list of list of tuple -- (word, prob) for all topk
		'''
		raise NotImplementedError

	def postprocess(self, token):
		"""
		clean up the tokens from any special chars and encode
		leading space by UTF-8 code '\u0120', linebreak with UTF-8 code 266 '\u010A'
		:param token:  str -- raw token text
		:return: str -- cleaned and re-encoded token text
		"""
		raise NotImplementedError


def top_k_logits(logits, k):
	'''
	Filters logits to only the top k choices
	from https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py
	'''
	if k == 0:
		return logits
	values, _ = torch.topk(logits, k)
	min_values = values[:, -1]
	return torch.where(logits < min_values,
						 torch.ones_like(logits, dtype=logits.dtype) * -1e10,
						 logits)


@register_api(name='gpt2-l')
class LM(AbstractLanguageChecker):
	def __init__(self, model_name_or_path="gpt2-l"):
		super(LM, self).__init__()
		self.enc = GPT2Tokenizer.from_pretrained(model_name_or_path,cache_dir = '/rdata/zainsarwar865/models')
		self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path, cache_dir='/rdata/zainsarwar865/models')
		self.model.to(self.device)
		self.model.eval()
		self.start_token = '<|endoftext|>'
		print("Loaded GPT-2 model!")

	def check_probabilities(self, in_text, topk=40):
		# Process input
		start_t = torch.full((1, 1),
							 self.enc.encoder[self.start_token],
							 device=self.device,
							 dtype=torch.long)
		context = self.enc.encode(
							in_text,
							truncation=True,                      # Sentence to encode.
							max_length = 512,           # Pad & truncate all sentences.
							
							)
		context = torch.tensor(context,
								 device=self.device,
								 dtype=torch.long).unsqueeze(0)
		context = torch.cat([start_t, context], dim=1)
		# Forward through the model
	 
		with torch.no_grad():

			logits = self.model(context)
			# construct target and pred
			yhat = torch.softmax(logits[0][0, :-1], dim=-1)
			y = context[0, 1:]
			# Sort the predictions for each timestep
			sorted_preds = np.argsort(-yhat.data.cpu().numpy())
			# [(pos, prob), ...]
			real_topk_pos = list(
				[int(np.where(sorted_preds[i] == y[i].item())[0][0])
				 for i in range(y.shape[0])])
			real_topk_probs = yhat[np.arange(
				0, y.shape[0], 1), y].data.cpu().numpy().tolist()
			real_topk_probs = list(map(lambda x: round(x, 5), real_topk_probs))

			real_topk = list(zip(real_topk_pos, real_topk_probs))
			# [str, str, ...]
			bpe_strings = [self.enc.decoder[s.item()] for s in context[0]]

			bpe_strings = [self.postprocess(s) for s in bpe_strings]

			# [[(pos, prob), ...], [(pos, prob), ..], ...]
			pred_topk = [
				list(zip([self.enc.decoder[p] for p in sorted_preds[i][:topk]],
						 list(map(lambda x: round(x, 5),
									yhat[i][sorted_preds[i][
											:topk]].data.cpu().numpy().tolist()))))
				for i in range(y.shape[0])]

		pred_topk = [[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]
		payload = {'bpe_strings': bpe_strings,
					 'real_topk': real_topk,
					 'pred_topk': pred_topk}
		if torch.cuda.is_available():
			torch.cuda.empty_cache()

		return payload

	def postprocess(self, token):
		with_space = False
		with_break = False
		if token.startswith('Ġ'):
			with_space = True
			token = token[1:]
			# print(token)
		elif token.startswith('â'):
			token = ' '
		elif token.startswith('Ċ'):
			token = ' '
			with_break = True

		token = '-' if token.startswith('â') else token
		token = '“' if token.startswith('ľ') else token
		token = '”' if token.startswith('Ŀ') else token
		token = "'" if token.startswith('Ļ') else token

		if with_space:
			token = '\u0120' + token
		if with_break:
			token = '\u010A' + token

		return token




@register_api(name='bert-large-cased')
class BERTLM(AbstractLanguageChecker):
	def __init__(self, model_name_or_path="bert-large-cased"):
		super(BERTLM, self).__init__()
		self.device = torch.device(
			"cuda" if torch.cuda.is_available() else "cpu")
		self.tokenizer = BertTokenizer.from_pretrained(
			model_name_or_path,
			do_lower_case=False,
			cache_dir='/rdata/zainsarwar865/models')
		self.model = BertForMaskedLM.from_pretrained(
			model_name_or_path,cache_dir='/rdata/zainsarwar865/models')
		self.model.to(self.device)
		self.model.eval()
		# BERT-specific symbols
		self.mask_tok = self.tokenizer.convert_tokens_to_ids(["[MASK]"])[0]
		self.pad = self.tokenizer.convert_tokens_to_ids(["[PAD]"])[0]
		print("Loaded BERT model!")

	def check_probabilities(self, in_text, topk=40, max_context=20,
							batch_size=20):
		'''
		Same behavior as GPT-2
		Extra param: max_context controls how many words should be
		fed in left and right
		Speeds up inference since BERT requires prediction word by word
		'''
		
		y_toks = (self.tokenizer.encode_plus(in_text,
											 max_length=512,
											 truncation=True,
										add_special_tokens=True))['input_ids']
		# Construct target
		#y_toks = self.tokenizer.convert_tokens_to_ids(tokenized_text)
		# Only use sentence A embedding here since we have non-separable seq's
		
		in_text_ = "[CLS] " + in_text + " [SEP]"
		tokenized_text = self.tokenizer.tokenize(in_text)
		segments_ids = [0] * len(y_toks)
		y = torch.tensor([y_toks]).to(self.device)
		segments_tensor = torch.tensor([segments_ids]).to(self.device)

		# TODO batching...
		# Create batches of (x,y)
		input_batches = []
		target_batches = []
		for min_ix in range(0, len(y_toks), batch_size):
			max_ix = min(min_ix + batch_size, len(y_toks) - 1)
			cur_input_batch = []
			cur_target_batch = []
			# Construct each batch
			for running_ix in range(max_ix - min_ix):
				tokens_tensor = y.clone()
				mask_index = min_ix + running_ix
				tokens_tensor[0, mask_index + 1] = self.mask_tok

				# Reduce computational complexity by subsetting
				min_index = max(0, mask_index - max_context)
				max_index = min(tokens_tensor.shape[1] - 1,
								mask_index + max_context + 1)

				tokens_tensor = tokens_tensor[:, min_index:max_index]
				# Add padding
				needed_padding = max_context * 2 + 1 - tokens_tensor.shape[1]
				if min_index == 0 and max_index == y.shape[1] - 1:
					# Only when input is shorter than max_context
					left_needed = (max_context) - mask_index
					right_needed = needed_padding - left_needed
					p = torch.nn.ConstantPad1d((left_needed, right_needed),
												 self.pad)
					tokens_tensor = p(tokens_tensor)
				elif min_index == 0:
					p = torch.nn.ConstantPad1d((needed_padding, 0), self.pad)
					tokens_tensor = p(tokens_tensor)
				elif max_index == y.shape[1] - 1:
					p = torch.nn.ConstantPad1d((0, needed_padding), self.pad)
					tokens_tensor = p(tokens_tensor)

				cur_input_batch.append(tokens_tensor)
				cur_target_batch.append(y[:, mask_index + 1])
				# new_segments = segments_tensor[:, min_index:max_index]
			if(len(cur_input_batch) == 0 or len(cur_target_batch) == 0):
				continue
			else:
				cur_input_batch = torch.cat(cur_input_batch, dim=0)
				cur_target_batch = torch.cat(cur_target_batch, dim=0)
				input_batches.append(cur_input_batch)
				target_batches.append(cur_target_batch)

		real_topk = []
		pred_topk = []

		with torch.no_grad():
			for src, tgt in zip(input_batches, target_batches):
				# Compute one batch of inputs
				# By construction, MASK is always the middle
				logits = self.model(src, torch.zeros_like(src))[0][:,
						 max_context + 1]
				yhat = torch.softmax(logits, dim=-1)

				sorted_preds = np.argsort(-yhat.data.cpu().numpy())
				# TODO: compare with batch of tgt

				# [(pos, prob), ...]
				real_topk_pos = list(
					[int(np.where(sorted_preds[i] == tgt[i].item())[0][0])
					 for i in range(yhat.shape[0])])
				real_topk_probs = yhat[np.arange(
					0, yhat.shape[0], 1), tgt].data.cpu().numpy().tolist()
				real_topk.extend(list(zip(real_topk_pos, real_topk_probs)))

				# # [[(pos, prob), ...], [(pos, prob), ..], ...]
				pred_topk.extend([list(zip(self.tokenizer.convert_ids_to_tokens(
					sorted_preds[i][:topk]),
					yhat[i][sorted_preds[i][
							:topk]].data.cpu().numpy().tolist()))
					for i in range(yhat.shape[0])])

		bpe_strings = [self.postprocess(s) for s in tokenized_text]
		pred_topk = [[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]
		payload = {'bpe_strings': bpe_strings,
					 'real_topk': real_topk,
					 'pred_topk': pred_topk}
		return payload

	def postprocess(self, token):

		with_space = True
		with_break = token == '[SEP]'
		if token.startswith('##'):
			with_space = False
			token = token[2:]

		if with_space:
			token = '\u0120' + token
		if with_break:
			token = '\u010A' + token
		#
		# # print ('....', token)
		return token


#========================================================================

def compute_metrics(preds,labels,y_true,y_scores, model):

	precision, recall, fscore, support = score(labels, preds)
	auc_train = roc_auc_score(y_true, y_scores)
	return {"model": model,
			"AUC": auc_train,
			"acc": np.mean(preds == labels),
			"precision_machine": precision[1],
			"recall_machine": recall[1],
			"fscore_machine":fscore[1],
			"support_machine":float(support[1]),
			"precision_human":precision[0],
			"recall_human": recall[0],
			"fscore_human":fscore[0],
			"support_human":float(support[0]),
			}




def main():

	parser = argparse.ArgumentParser()
	parser.add_argument(
		'--train_dataset', # training dataset
		default=None,
		type=str,
		required=True,
		 )

	parser.add_argument(
		'--bert_large_gltr_ckpt', # The path to save fitted model checkpoint
		default=None,
		type=str,
		required=True,
		 )

	parser.add_argument(
		'--bert_model', # backend model
		default=None,
		type=str,
		required=True,
		 )

	parser.add_argument(
		'--output_metrics', # the path to save the training performance metrics
		default=None,
		type=str,
		required=True,
		 )
	
	args = parser.parse_args()


	histogram_of_likelihoods_train =  []
	labels_train = []

	lm = BERTLM(model_name_or_path=args.bert_model)
	with jsonlines.open(args.train_dataset, 'r') as src_file:
		for article in src_file:
			print("Processing article number: {}".format(article['id']))
			raw_text = article['text']
			labels_train.append(article['label'])
			payload = lm.check_probabilities(raw_text, topk=5)
			count_10 = 0
			count_100 = 0
			count_1000 = 0
			count_beyond_1000 = 0
			payload = np.asarray(payload['real_topk'])
			for i in range(payload.shape[0]):
				rank = payload[i][0].astype(np.int64)
				if rank <=9:
					count_10 +=1
				elif rank <= 99:
					count_100+=1
				elif rank <= 999:
					count_1000+=1
				else:
					count_beyond_1000+=1

			histogram_of_likelihoods_train.append([count_10,count_100,count_1000,count_beyond_1000])

	labels_train = np.asarray(labels_train)
	labels_train = np.where((labels_train == 'machine'),1,0)


	# ###############save best model#############
	# define models and parameters
	model = LogisticRegression()
	solvers = ['newton-cg', 'lbfgs', 'liblinear']
	penalty = ['l2']
	c_values = [100, 10, 1.0, 0.1, 0.01]
	# define grid search
	grid = dict(solver=solvers, penalty=penalty, C=c_values)
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
	grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)
	grid_result = grid_search.fit(histogram_of_likelihoods_train, labels_train)


	model_train = grid_result.best_estimator_




	y_preds_train = model_train.predict(histogram_of_likelihoods_train)
	y_train_probs = model_train.predict_proba(histogram_of_likelihoods_train)

	acc = np.mean(y_preds_train == labels_train)
	print("Accuracy on bert on training set is : {}".format(acc))
	bert_metrics = compute_metrics(y_preds_train,labels_train,labels_train, y_train_probs[:,1],args.bert_model)

	filename_train = args.bert_large_gltr_ckpt
	pickle.dump(model_train, open(filename_train, 'wb'))

	with jsonlines.open(args.output_metrics, 'w') as output_metrics:
		output_metrics.write(bert_metrics)


if __name__ == '__main__':
	main()

